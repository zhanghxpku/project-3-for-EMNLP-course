---

# Dataset metadata
default_dataset: &default_dataset
    type: PythonDataset
    delim: "\t"
    buffer_size: 40000
    shuffle: False
    map_tables:
        word2id:
            path: word.dict.emb
        relation2id: 
            path: relation.single.dict

    slots: 
        idx:
            index: 0
            type: value
            value_type: int32
        word: &default_slot
            index: 1
            type: sequence
            delim: " "
            map_table: word2id
            max_length: 46
            pad: 0
        relation:
            index: 2
            type: value
            map_table: relation2id
            << : *default_slot
        entity:
            index: 3
            max_length: 47
            << : *default_slot
            

pred_dataset: &pred_dataset
    type: PythonDataset
    delim: "\t"
    buffer_size: 40000
    shuffle: True
    map_tables:
        path: word2.dict

    slots: 
        idx:
            index: 0
            type: value
            value_type: int32
        word: 
            index: 1
            type: sequence
            delim: " "
            map_table: word2id
            max_length: 46
            pad: 0
#        relation:
#            index: 2
#            max_length: 12
#            << : *default_slot
        entity:
            index: 2
            max_length: 47
            << : *default_slot

train_dataset: &train_dataset
    path: data/EMNLP.train.single.new
    << : *default_dataset

dev_dataset: &dev_dataset
    shuffle: False
    path: data/EMNLP.dev.single.new
    << : *default_dataset

test_dataset: &test_dataset
    shuffle: False
    path: data/EMNLP.dev.single.new
    << : *default_dataset

predict_dataset: &predict_dataset
    shuffle: False
    path: data/EMNLP.test.single.new
    << : *pred_dataset

# logging:
#     file: results/lstm_semantic_parsing/logging.out
#     level: 3

# Model config
model:
    model_name: LSTMDependencyModel
    
    # Data
    use_word_pretrain_emb: False
    word_emb_finetune: True
    word2vec_dict: 'embedding.300d'
#    relation2grams: 'relation2grams.npy'
    relation2words: 'relation2words_emb.npy'
    max_length: 46
    max_char: 31
    
    vocab_size: 15456
#    trigram_size: 7888
    relation_size: 1213
#    relation_max_char: 17
    relation_max_len: 12
    
    # Network
#    region_radius: 1
    emb_size: 300
    hidden_size: 500
    semantic_size: 500
    dropout_rate: 0.3
    negative_samples: 1213
    gamma: 5
    
    # Training
    batch_size: 100
    optimizer: Adam
    learning_rate: 0.02
    use_clip_by_norm: True
    decay_step: 270
    decay_rate: 0.9
#    metric: EMMetric

# Estimator
estimator:
    type: PythonEstimator
    train_dataset: *train_dataset
    eval_datasets:
        # - *train_dataset
        - *dev_dataset
        # - *test_dataset
    eval_to_file: True
    eval_op_path: results/lstm_semantic_parsing/eval.output
    # infer_dataset: *predict_dataset
    # infer_op_path: results/lstm_semantic_parsing/infer.output

    batch_size: 100
    max_epochs: 40

    word2id: word.dict.emb
    relation2id: relation.single.dict
    checkpoint_dir: results/lstm_semantic_parsing/checkpoint

    model_name: semantic_parsing_model
    # save_checkpoints_steps: 2000
    eval_interval_epochs: 100
#    eval_interval_steps: 100
    max_training_steps: 40000
    log_every_n_steps: 20
    tolerance: 5
    skip_eval_save: False