---

# Dataset metadata
default_dataset: &default_dataset
    type: PythonDataset
    delim: "\t"
    buffer_size: 40000
    shuffle: True
    map_tables:
        char2id:
            path: char.dict
        gram2id:
            path: trigram.dict
        word2id:
            path: word_char.dict
        word2id_emb:
            path: word_emb.dict
        word2id_train:
            path: word.dict
        relation2id: 
            path: relation.dict
        entity2id: 
            path: entity.dict
        comb2id: 
            path: comb.dict

    slots: 
        idx:
            index: 0
            type: value
            value_type: int32
#        char: &default_slot
#            index: 1
#            type: sequence
#            delim: " "
#            map_table: char2id
#            max_length: 264 # 389
#            pad: 0
#        gram:
#            index: 2
#            map_table: gram2id
#            << : *default_slot
        word_all: &default_slot
            index: 1
            type: sequence
            delim: " "
            max_length: 60
            map_table: word2id
            pad: 0
            << : *default_slot
        word_emb:
            index: 2
#            max_length: 60
            map_table: word2id_emb
            << : *default_slot
        word_train:
            index: 2
#            max_length: 60
            map_table: word2id_train
            << : *default_slot
        relation:
            index: 3
            type: value
            map_table: comb2id
            << : *default_slot
        entity:
            index: 4
            max_length: 2
            map_table: entity2id
            << : *default_slot
        typ:
            index: 5
            type: value
            value_type: int32
        entity_order:
            index: 6
            type: value
            value_type: int32
        # Word
        entity_mask1:
            index: 7
#            max_length: 60
            value_type: float32
            << : *default_slot
        entity_mask2:
            index: 8
#            max_length: 60
            value_type: float32
            << : *default_slot

pred_dataset: &pred_dataset
    type: PythonDataset
    delim: "\t"
    buffer_size: 40000
    shuffle: True
    map_tables:
        word2id:
            path: word.dict
        word2id_emb:
            path: word_emb.dict
        word2id_train:
            path: word_train.dict
        relation2id: 
            path: relation.dict

    slots: 
        idx:
            index: 0
            type: value
            value_type: int32
        word:
            index: 1
            << : *default_slot
        word_emb:
            index: 3
            map_table: word2id_emb
            << : *default_slot
        word_train:
            map_table: word2id_train
            << : *default_slot
        word_all:
            map_table: word2id_all
            << : *default_slot
#        relation:
#            index: 2
#            type: value
#            map_table: relation2id
#            << : *default_slot
        entity:
            index: 4
            type: value
            map_table: entity2id

train_dataset: &train_dataset
    path: data/EMNLP.train.new
    << : *default_dataset

dev_dataset: &dev_dataset
    shuffle: False
    path: data/EMNLP.dev.new
    << : *default_dataset

test_dataset: &test_dataset
    shuffle: False
    path: data/EMNLP.dev.new
    << : *default_dataset

#predict_dataset: &predict_dataset
#    shuffle: False
#    path: data/EMNLP.test.new
#    << : *pred_dataset

# logging:
#     file: results/semantic_parsing/logging.out
#     level: 3

# Model config
model:
    model_name: DependencyModel
    
    # Data
    relation_max_char: 94
    relation_max_len: 12
    
    char_size: 206
    trigram_size: 9416
    vocab_size: 21330
    vocab_size_emb: 16908
    vocab_size_train: 18490
    relation_size: 1562
    comb_size: 436
    single_size: 1229

    # Word-level encoder
    word:
        use_word_pretrain_emb: True
        word_emb_finetune: True
        word2vec_emb: 'embedding.300d'
        relation2word: 'relation2word.npy'
        relation2word_emb: 'relation2word_emb.npy'
        relation2word_all: 'relation2word_all.npy'
    #    relation2word_train: 'relation2word_train.npy'
        emb_size: 300
        encoder: none # none, mean, region, CNN
        region_radius: 2 # only for mean and region
        groups: 1 # only for CNN
        filters: # only for CNN
            - 50
        kernel_size: # only for CNN
            - 3

    # Char-level encoder
    char:
        emb_size: 1000
        relation2char: 'relation2char.npy'
        relation2gram: 'relation2gram.npy'
        word2char: 'word2char.npy'
        word2gram: 'word2gram.npy'
        encoder_type: trigram # trigram or char
        encoder: mean # none, mean, region, CNN
        region_radius: 2 # only for mean and region
        groups: 3 # only for CNN
        filters: # only for CNN
            - 300
            - 300
            - 300
        kernel_size: # only for CNN
            - 3
            - 3
            - 3
        use_rnn: False
        hidden_size: 500 # only for use_rnn
        aggregation: mean # max, mean, attention

    # Combination
    combination:
        use_highway: False
        use_rnn: False
        hidden_size: 500 # only for use_rnn
        aggregation: max # max, mean, end (only for use_rnn), attention
    
    # Projection to semantic space
    layer_num: 0
    semantic_size: 1000
    gamma: 5
    comb2relation: 'comb2relation.npy'
    
    # Training
    batch_size: 100
    optimizer: Adam
    learning_rate: 0.001
    dropout_rate: 0.05
#    negative_samples: 100
    use_clip_by_norm: True
    decay_step: 309
    decay_rate: 0.9
    train_order: False

# Estimator
estimator:
    type: PythonEstimator
    train_dataset: *train_dataset
    eval_datasets:
#        - *train_dataset
        - *dev_dataset
        # - *test_dataset
    eval_to_file: True
    eval_op_path: results/semantic_parsing/eval.output
    # infer_dataset: *predict_dataset
    # infer_op_path: results/semantic_parsing/infer.output

    batch_size: 100
    max_epochs: 40

    char2id: char.dict
    word2id: word.dict
    word2id_emb: word_emb.dict
    relation2id: relation.dict
    comb2id: comb.dict
    entity2id: entity.dict
    checkpoint_dir: results/semantic_parsing/checkpoint

    model_name: semantic_parsing_model
    # save_checkpoints_steps: 2000
    eval_interval_epochs: 100
#    eval_interval_steps: 100
    max_training_steps: 31000
    log_every_n_steps: 20
    tolerance: 5
    skip_eval_save: False